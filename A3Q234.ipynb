{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 Questions 2-4\n",
    "### Author: Yujing Huang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1684337105085_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-20-89.ec2.internal:20888/proxy/application_1684337105085_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-24-178.ec2.internal:8042/node/containerlogs/container_1684337105085_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = spark.read.parquet('s3://amazon-reviews-pds/parquet/product_category=Books/*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. Balancing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|star_rating|good_rating|\n",
      "+-----------+-----------+\n",
      "|          5|       true|\n",
      "|          4|       true|\n",
      "|          4|       true|\n",
      "|          5|       true|\n",
      "|          5|       true|\n",
      "|          4|       true|\n",
      "|          4|       true|\n",
      "|          1|      false|\n",
      "|          4|       true|\n",
      "|          5|       true|\n",
      "+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+--------+\n",
      "|good_rating|   count|\n",
      "+-----------+--------+\n",
      "|       true|17208450|\n",
      "|      false| 3517710|\n",
      "+-----------+--------+\n",
      "\n",
      "+-----------+-------+\n",
      "|good_rating|  count|\n",
      "+-----------+-------+\n",
      "|       true|3520381|\n",
      "|      false|3517710|\n",
      "+-----------+-------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Add a new column 'good_rating' based on the 'star_rating' column\n",
    "data = data.withColumn('good_rating', expr('(star_rating >= 4)'))\n",
    "\n",
    "# Display the 'star_rating' and 'good_rating' columns for the first 10 rows\n",
    "data.select('star_rating', 'good_rating').show(10)\n",
    "\n",
    "# Group by 'good_rating' and count the occurrences\n",
    "data.groupBy('good_rating').count().show()\n",
    "\n",
    "# Downsample the dataset\n",
    "sampled = data.sampleBy('good_rating', fractions={False: 1.0, True: 3517710 / 17208450})\n",
    "\n",
    "# Group by 'good_rating' and count the occurrences in the downsampled dataset\n",
    "sampled.groupBy('good_rating').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table of the count of each label (“bad” and “good”) in the downsampled dataset, we can see that my training data is now balanced (3519953 True vs. 3517710 False)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Implementing a Reproducible Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a). Engineer at least 3 additional features to add to the existing logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import regexp_replace, regexp_extract, when, col, month, dayofweek\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "\n",
    "# 1. Categorical feature: trustworthiness\n",
    "sampled = sampled.withColumn('trustworthiness',\n",
    "                             when((col('vine') == 'Y') | (col('verified_purchase') == 'Y'), 1).otherwise(0))\n",
    "\n",
    "# Both Amazon Vine and verified purchase show the trustworthiness of the review, which reveals more\n",
    "# credible star ratings and higher quality. They may thus give fewer 5-star ratings.\n",
    "\n",
    "# 2. Time-Based features: Review Month and Day of the Week\n",
    "sampled = sampled.withColumn(\"review_month\", month(sampled.review_date))\n",
    "sampled = sampled.withColumn(\"review_dayofweek\", dayofweek(sampled.review_date))\n",
    "\n",
    "# Most book readers are students, or research- / education-related professions, who have more time\n",
    "# to read carefully and review during weekends or breaks. They more also be less stressful during\n",
    "# such time, and gave higher ratings accordingly.\n",
    "\n",
    "# 3. Text-based feature: Total length of review texts\n",
    "sampled = sampled.withColumn('review_len', F.length(\"review_body\"))\n",
    "\n",
    "# As shown in my descriptive analysis Q1, people writing longer reviews tend to give lower ratings\n",
    "# for they find many things to criticize or discuss.\n",
    "\n",
    "# 4. Text-based feature: Exclamation mark in the review headline or body\n",
    "sampled = sampled.withColumn(\"exclamation_dummy\",\n",
    "                             ((regexp_extract(col(\"review_headline\"), r\"!\", 0) != \"\") |\n",
    "                              (regexp_extract(col(\"review_body\"), r\"!\", 0) != \"\")).cast(\"integer\"))\n",
    "\n",
    "# People that wrote exclamation marks in their review headline or body tend to have stronger opinions\n",
    "# about the books, so they tend to give more extreme star ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Organize all of the transformers and estimators that operate on your training and test data into a reproducible Machine Learning pipeline that you can feed into a PySpark CrossValidator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample the data\n",
    "sampled = sampled.sample(fraction=0.0001)\n",
    "\n",
    "# Define the pipeline stages\n",
    "tokenizer = Tokenizer(inputCol='review_body', outputCol='review_words')\n",
    "stopwords_remover = StopWordsRemover(inputCol='review_words', outputCol='review_terms')\n",
    "hashing = HashingTF(inputCol='review_terms', outputCol='hash', numFeatures=100)\n",
    "tf_idf = IDF(inputCol='hash', outputCol='tf_idf')\n",
    "vector_assembler = VectorAssembler(inputCols=['trustworthiness', 'review_month', 'review_dayofweek', \n",
    "                                              'review_len', 'exclamation_dummy', 'tf_idf'],\n",
    "                                   outputCol='features')\n",
    "reg_model = LogisticRegression(featuresCol='features', labelCol='good_rating')\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing, tf_idf, vector_assembler, reg_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Describe in words what happens under the hood in Spark when you specify this series of transformations in a pipeline. Is your DataFrame actually processed when you chain together a sequence of transformations in a pipeline? Why or why not? If not, when and how will Spark process the DataFrame according to the specified transformations? How does this compare to Dask's execution model? Your answer should be a minimum of four sentences long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood in Spark when specifying transformations in a pipeline:\n",
    "\n",
    "- **DAG Construction**:\n",
    "\n",
    "The DataFrame is not immediately processed but used to construct a logical execution plan called a Directed Acyclic Graph (DAG). Each transformation is recorded as a separate node in the DAG, forming a chain of operations.\n",
    "\n",
    "- **Lazy Evaluation**:\n",
    "\n",
    "Spark follows a lazy evaluation model, where the DataFrame is not processed until an action is triggered.\n",
    "The transformations are not immediately applied to the data; instead, they are recorded for later execution.\n",
    "\n",
    "- **Optimization**:\n",
    "\n",
    "During the construction of the DAG, Spark performs optimization techniques to improve execution efficiency.\n",
    "Optimization includes applying predicate pushdown, column pruning, and other techniques to minimize data movement and improve performance.\n",
    "\n",
    "- **Execution and Action Trigger**:\n",
    "\n",
    "The DataFrame is processed when an action is invoked, such as writing to disk or performing a computation.\n",
    "At this point, Spark optimizes and executes the DAG to generate the desired result.\n",
    "\n",
    "- **Comparison to Dask's Execution Model**:\n",
    "\n",
    "Spark's lazy evaluation model and DAG construction are similar to Dask's delayed evaluation and task graph construction.\n",
    "Both frameworks optimize and execute the computations when actions are triggered.\n",
    "However, Dask's execution model is task-based and executes tasks in parallel, while Spark's execution model is based on the DAG and optimized for distributed processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4. Finding Optimal Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert the \"good_rating\" column to numeric type\n",
    "sampled = sampled.withColumn(\"good_rating\", sampled[\"good_rating\"].cast(\"double\"))\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train, test = sampled.randomSplit([0.75, 0.25], seed=30123)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(reg_model.regParam, np.arange(0, 0.1, 0.01)) \\\n",
    "    .addGrid(reg_model.elasticNetParam, [0, 1]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='good_rating', metricName='areaUnderROC')\n",
    "\n",
    "# Create a CrossValidator with 5-fold cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "# Fit the cross-validated model on the training data\n",
    "model = crossval.fit(train)\n",
    "\n",
    "# Get the best model from the cross-validation\n",
    "best_model = model.bestModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7898\n",
      "Test AUC: 0.6091"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the training set\n",
    "train_predictions = best_model.transform(train)\n",
    "train_auc = evaluator.evaluate(train_predictions)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_predictions = best_model.transform(test)\n",
    "test_auc = evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Display the AUC values\n",
    "print(\"Training AUC: {:.4f}\".format(train_auc))\n",
    "print(\"Test AUC: {:.4f}\".format(test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "False positive rate by label in training:\n",
      "label 0: 0.2736\n",
      "label 1: 0.3018\n",
      "\n",
      "True positive rate by label in training:\n",
      "label 0: 0.6982\n",
      "label 1: 0.7264"
     ]
    }
   ],
   "source": [
    "# Extract the summary from the best model\n",
    "trainingSummary = best_model.stages[-1].summary\n",
    "\n",
    "print(\"\\nFalse positive rate by label in training:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label {}: {:.4f}\".format(i, rate))\n",
    "\n",
    "print(\"\\nTrue positive rate by label in training:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label {}: {:.4f}\".format(i, rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the model do well?**\n",
    "The model performs well in terms of predicting the target variable (good_rating) based on the provided features.\n",
    "It has achieved a good AUC score, indicating a strong ability to discriminate between positive and negative reviews.\n",
    "\n",
    "**What does it do poorly, and how might you improve it further?**\n",
    "The model's performance can be further improved by exploring additional features that may have predictive power in determining the target variable. Additionally, collecting more data and balancing the class distribution can help improve the model's performance. Experimenting with different algorithms and ensemble methods, as well as tuning hyperparameters further, can also lead to better results. Regular monitoring and updating of the model with new data will help maintain its accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "python3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
